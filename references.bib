@article{radford2018improving,
	title        = {Improving language understanding by generative pre-training},
	author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
	year         = 2018,
	publisher    = {OpenAI}
}
@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@article{nadaraya1964estimating,
	title        = {On estimating regression},
	author       = {Nadaraya, Elizbar A},
	year         = 1964,
	journal      = {Theory of Probability \& Its Applications},
	publisher    = {SIAM},
	volume       = 9,
	number       = 1,
	pages        = {141--142}
}
@article{watson1964smooth,
	title        = {Smooth regression analysis},
	author       = {Watson, Geoffrey S},
	year         = 1964,
	journal      = {Sankhy: The Indian Journal of Statistics, Series A},
	publisher    = {JSTOR},
	pages        = {359--372}
}
@article{mack1982weak,
	title        = {Weak and strong uniform consistency of kernel regression estimates},
	author       = {Mack, Yue-pok and Silverman, Bernard W},
	year         = 1982,
	journal      = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
	publisher    = {Springer},
	volume       = 61,
	pages        = {405--415}
}
@book{silverman1986density,
	title        = {Density estimation for statistics and data analysis},
	author       = {Silverman, Bernard W},
	year         = 1986,
	publisher    = {CRC press},
	volume       = 26
}
@article{bahdanau2014neural,
	title        = {Neural machine translation by jointly learning to align and translate},
	author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1409.0473}
}
@article{chan2015listen,
	title        = {Listen, attend and spell},
	author       = {Chan, William and Jaitly, Navdeep and Le, Quoc V and Vinyals, Oriol},
	year         = 2015,
	journal      = {arXiv preprint arXiv:1508.01211}
}
@book{goodfellow2016deep,
	title        = {Deep Learning},
	author       = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	year         = 2016,
	publisher    = {MIT Press},
	note         = {\url{http://www.deeplearningbook.org}}
}
@book{bishop2006pattern,
	title        = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	author       = {Bishop, Christopher M.},
	year         = 2006,
	publisher    = {Springer-Verlag},
	address      = {Berlin, Heidelberg},
	isbn         = {0387310738}
}
@article{zhang2021dive,
	title        = {Dive into Deep Learning},
	author       = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2106.11342}
}
@article{pinkus_1999,
	title        = {Approximation theory of the MLP model in neural networks},
	author       = {Pinkus, Allan},
	year         = 1999,
	journal      = {Acta Numerica},
	publisher    = {Cambridge University Press},
	volume       = 8,
	pages        = {143–195},
	doi          = {10.1017/S0962492900002919}
}
@misc{gripenberg2003approximation,
	title        = {Approximation by neural networks with a bounded number of nodes at each level},
	author       = {G. Gripenberg},
	year         = 2003,
	journal      = {Journal of Approximation Theory},
	volume       = 122,
	number       = 2,
	pages        = 260,
	doi          = {10.1016/s0021-9045(03)00078-9}
}
@inproceedings{mhaskar2017and,
	title        = {When and why are deep networks better than shallow ones?},
	author       = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
	year         = 2017,
	booktitle    = {Proceedings of the AAAI conference on artificial intelligence},
	volume       = 31,
	number       = 1
}
@article{voita2019analyzing,
	title        = {Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
	author       = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1905.09418}
}
@article{wolpert1997freelunch,
	title        = {No free lunch theorems for optimization},
	author       = {Wolpert, D.H. and Macready, W.G.},
	year         = 1997,
	journal      = {IEEE Transactions on Evolutionary Computation},
	volume       = 1,
	number       = 1,
	pages        = {67--82},
	doi          = {10.1109/4235.585893}
}
@article{ba2014deep,
	title        = {Do deep nets really need to be deep?},
	author       = {Ba, Jimmy and Caruana, Rich},
	year         = 2014,
	journal      = {Advances in neural information processing systems},
	volume       = 27
}
@inbook{nocedal2006,
	title        = {Line Search Methods},
	year         = 2006,
	booktitle    = {Numerical Optimization},
	publisher    = {Springer New York},
	address      = {New York, NY},
	pages        = {30--65},
	doi          = {10.1007/978-0-387-40065-5_3},
	isbn         = {978-0-387-40065-5},
	url          = {https://doi.org/10.1007/978-0-387-40065-5_3}
}
@article{polyak1964some,
	title        = {Some methods of speeding up the convergence of iteration methods},
	author       = {Polyak, Boris T},
	year         = 1964,
	journal      = {Ussr computational mathematics and mathematical physics},
	publisher    = {Elsevier},
	volume       = 4,
	number       = 5,
	pages        = {1--17}
}
@article{kingma2014adam,
	title        = {Adam: A method for stochastic optimization},
	author       = {Kingma, Diederik P and Ba, Jimmy},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1412.6980}
}
@article{luo2018towards,
	title        = {Towards understanding regularization in batch normalization},
	author       = {Luo, Ping and Wang, Xinjiang and Shao, Wenqi and Peng, Zhanglin},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1809.00846}
}
@inproceedings{he2016deep,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {770--778}
}
@article{kaplan2020scaling,
	title        = {Scaling laws for neural language models},
	author       = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2001.08361}
}
@article{schick2023toolformer,
	title        = {Toolformer: Language models can teach themselves to use tools},
	author       = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.04761}
}
@article{radford2019language,
	title        = {Language models are unsupervised multitask learners},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	year         = 2019,
	journal      = {OpenAI blog},
	volume       = 1,
	number       = 8,
	pages        = 9
}
@misc{gpt2trained,
	title        = {\textit{GPT2-spanish}: modelo pre-entrenado},
	note         = {Accessed: 2023-04-12},
	howpublished = {\url{https://huggingface.co/DeepESP/gpt2-spanish?doi=true}}
}
@article{wei2022emergent,
	title        = {Emergent abilities of large language models},
	author       = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.07682}
}
@article{dosovitskiy2020image,
	title        = {An image is worth 16x16 words: Transformers for image recognition at scale},
	author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2010.11929}
}
@article{bubeck2023sparks,
	title        = {Sparks of artificial general intelligence: Early experiments with gpt-4},
	author       = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.12712}
}
@article{srivastava2014dropout,
	title        = {Dropout: a simple way to prevent neural networks from overfitting},
	author       = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year         = 2014,
	journal      = {The journal of machine learning research},
	publisher    = {JMLR. org},
	volume       = 15,
	number       = 1,
	pages        = {1929--1958}
}
@article{brohan2022rt,
	title        = {Rt-1: Robotics transformer for real-world control at scale},
	author       = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2212.06817}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}
@article{phuong2022formal,
  title={Formal algorithms for transformers},
  author={Phuong, Mary and Hutter, Marcus},
  journal={arXiv preprint arXiv:2207.09238},
  year={2022}
}
@misc{gpthallucination,
	title        = {Hallucinations Could Blunt ChatGPT’s Success},
	note         = {Accessed: 2023-06-12},
	howpublished = {\url{https://spectrum.ieee.org/ai-hallucination}}
}

@misc{gptthreats,
	title        = {The New AI-Powered Bing Is Threatening Users.},
	note         = {Accessed: 2023-06-12},
	howpublished = {\url{https://time.com/6256529/bing-openai-chatgpt-danger-alignment/}}
}


