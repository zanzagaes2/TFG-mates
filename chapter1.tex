\cleardoublepage

\chapter{Introducción}
Probablemente el lector ha interactuado con sistemas como Chat-GPT o Bard, asistentes capaces de conversar convincentemente sobre una variedad de temas, generar código o resolver exámenes. Todos estos sistemas tienen por núcleo redes neuronales profundas llamados \textit{grandes modelos de lenguaje} con una arquitectura concreta, la arquitectura \textit{transformer}, presentada por primera vez en 2017 \cite{vaswani2017attention}.

Desde su aparición, los \textit{transformer} se han convertido en la opción preferencial para abordar problemas relacionados con el procesamiento de lenguaje natural, por su excelente rendimiento y porque presentan una arquitectura modular escalable, que permite construir modelos más grandes y potentes sin demasiados inconvenientes. Estos modelos pueden adaptarse a resolver problemas no textuales, como la clasificación de imágenes, obteniendo en muchas ocasiones resultados superiores a las de arquitecturas especializadas \cite{dosovitskiy2020image}.

Lo verdaderamente sorprendente, no obstante, es que, una vez alcanzan una escala suficiente, estos modelos empiezan a mostrar \textit{habilidades emergentes}, volviéndose capaces de realizar tareas para las que no han sido entrenados, como seguir listas de instrucciones o realizar operaciones aritméticas \cite{wei2022emergent}. Los grandes modelos de lenguaje ya presentan un rendimiento mayor que los humanos en una multitud de tareas (por ejemplo, obtienen una calificación en el percentil 90 en el examen de acceso universitario SAT) y son considerados por algunos profesionales como el preámbulo de las inteligencias artificiales de propósito general \cite{bubeck2023sparks}.

Estos modelos no sólo tienen un interés teórico. Aunque el proceso de integración de los \textit{transformers} no ha hecho más que empezar, estos modelos ya se utilizan como traductores automáticos y asistentes conversacionales y se han integrado en los buscadores. 

Conforme se generalizan este tipo de redes, sus limitaciones se hacen más claras y preocupantes: pueden presentar de forma convincente resultados inventados (añadiendo citas y referencias inexistentes), ser engañados para desoír las indicaciones dadas por sus programadores o llegar a amenazar a los usuarios \cite{gpthallucination} \cite{gptthreats}. Su capacidad para ser confundidos con humanos, las convierte en temibles armas para la desinformación, la propaganda política o la publicidad. 

Existen, por tanto, razones más que sobradas para justificar el estudio de las \textit{transformers} y quienes no se vean atraídos por comprender la elegancia de su diseño o lo amplio de sus posibilidades, coincidirán en la necesidad de comprender las amenazas y oportunidades que plantea su integración en nuestra vida diaria.

\section{Objetivos del trabajo}
La velocidad de avance del campo del aprendizaje profundo y lo relativamente aislado de su comunidad, ha provocado un grave desfase entre la realidad del campo y los contenidos cubiertos por los manuales y otros recursos. Los manuales existentes sobre el tema, aunque de una calidad excelente, no asumen conocimientos matemáticos extensos previos y deben cubrir una inmensa cantidad de contenido. Además, la mayoría son anteriores a la publicación de los \textit{transformers}.

Este trabajo se ha estructurado para ofrecer una introducción al campo del aprendizaje profundo y, específicamente, a las arquitecturas \textit{transformers} que requiere únicamente conocimiento matemático sólido en los fundamentos de la teoría de la probabilidad, la inferencia estadística, el álgebra lineal y el cálculo multivariable, sin que sea necesario ningún conocimiento previo sobre redes neuronales.

A fin de dar una aplicación práctica a los conceptos, el trabajo finaliza con una implementación del modelo GPT (probablemente la aplicación más influyente de la arquitectura \textit{transformer}) y su uso para abordar un problema complejo: crear una red neuronal capaz de generar texto al estilo de Lope de Vega. Por lo que sabemos, esta es la primera vez que se entrena una red similar para reproducir el estilo de un autor en lengua castellana.

En conclusión, las metas fundamentales del trabajo son:
\begin{enumerate}[label=(O\arabic*)]
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \item Presentar los conceptos generales sobre redes neuronales imprescindibles: qué es una red neuronal, qué propiedades tiene y cómo puede entrenarse.
    \item Presentar el problema de procesamiento de lenguaje natural y sus singularidades. Realizar una revisión del estado de la cuestión.
    \item Presentar la arquitectura \textit{transformer} y ponerla en relación con arquitecturas anteriores. Entender cómo puede modificarse para abordar problemas diversos, como la generación de texto
    \item Implementar \textit{GPT} y utilizar la arquitectura para abordar el problema de generar texto en el estilo de Lope de Vega. Evaluar el rendimiento y discutir alternativas y limitaciones.
\end{enumerate}