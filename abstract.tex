
\section*{Abstract}
Since its debut in 2017, the \textit{transformer} architecture has emerged as the standard for designing deep neural networks in the field of natural language processing. This architecture, comprised of modular blocks that can be combined to create highly expansive neural networks, serves as the foundation for cutting-edge models such as ChatGPT and Bing Chat.

Transformer-based models have consistently demonstrated remarkable capabilities in addressing diverse problems, including text generation, question answering or logical reasoning. They have even surpassed human performance in various tasks, leading researchers to observe ``sparks of a general artificial intelligence'' in these models.

Despite the active research being conducted in this field, there has been limited effort to provide a mathematically formal, systematic, and concise description of transformers. Most existing machine learning manuals were published prior to the advent of transformers, prioritize intuition over mathematical precision, or cover a wide range of concepts as a general introduction to the field. This work aims to offer a reasonably concise and mathematically precise introduction to transformer architectures without assuming any prior knowledge of neural networks.

To provide a proper context for the innovations found in transformer models, we present a comprehensive review of deep learning techniques in natural language processing. This text covers the latest advancements and can serve as an introduction to current research in the field.

Furthermore, we demonstrate the practical implementation of these theoretical concepts by focusing on one of the most successful families of transformer-based models, GPT. We tackle a significant challenge by utilizing GPT to generate text in the style of comedic plays by Lope de Vega. The text includes both pseudocode and actual code required to implement, train, and evaluate the model.

\section*{Keywords}
Transformer, machine learning, deep learning, natural language processing, artificial intelligence, neural networks, encoder-decoder architecture, GPT, language modeling, text generation  