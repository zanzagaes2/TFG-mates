\section*{Resumen}
Desde su aparición en 2017, la arquitectura \textit{transformer} se ha convertido en la opción preferencial para diseñar redes neuronales para el procesamiento de lenguaje natural. La arquitectura, basada en bloques modulares que pueden combinarse para crear redes neuronales extremadamente grandes, es el núcleo de los modelos más avanzados hasta la fecha, como \textit{ChatGPT} o \textit{Bing Chat}.

Los modelos \textit{transformer} se han demostrado capaces de abordar excelentemente una clase muy diversa de problemas, incluyendos la generación de texto, la respuesta a preguntas o el razonamiento lógico. De hecho, ya superan a los seres humanos en una variedad de tareas, hasta el punto de que algunos investigadores consideran que algunos de estos modelos muestran ``destellos de una inteligencia artifical de propósito general''\cite{bubeck2023sparks}.

Si bien el campo es objeto de una intensa actividad investigadora, no hay todavía textos de referencia que presenten una descripción matemáticamente formal, sistemática y concisa de los \textit{transformer}. La mayoría de los manuales sobre \textit{aprendizaje automático} son anteriores a la aparición de los \textit{transformer}, priorizan la intuición sobre la precisión matemática o introducen un gran número de conceptos, en el intento de servir como una introducción generalista al campo. Este trabajo presenta una introducción razonablemente concisa y matemáticamente precisa de las arquitecturas \textit{transformer} y de su ejemplo más exitoso, la familia GPT, que no requiere conocimientos previos sobre redes neuronales. 

Con el fin de enmarcar adecuadamente las innovaciones presentes en los modelos \textit{transformer}, presentamos una revisión exhaustiva del campo del procesamiento del lenguaje natural utilizando técnicas de aprendizaje profundo. Al cubrir los avances más recientes, este texto bien puede servir como un preámbulo a la investigación actual en el campo.

Los conceptos teóricos introducidos en el trabajo son puestas en prácticas mediante la implementación de una de las familias de modelos \textit{transformer} más exitosa, GPT, y su uso para abordar un problema desafiante: la generación de texto al estilo de las comedias de Lope de Vega. Se presenta aquí tanto el pseudocódigo como el código concreto necesario para implementar, entrenar y evaluar el modelo.

\section*{Palabras clave}
Transformer, aprendizaje automático, aprendizaje profundo, procesamiento de lenguaje natural, inteligencia artificial, redes neuronales, arquitectura codificador-decodificador, GPT, modelado de lenguaje, generación de texto  

