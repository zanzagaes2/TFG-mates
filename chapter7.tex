\chapter{Conclusiones}
En este trabajo hemos introducido las redes neuronales y presentado con cierto detalle algunos de los problemas generales asociados, como su entrenamiento utilizando técnicas de gradiente o la regularización. El campo está dominado por las redes neuronales profundas, modelos con una excelente capacidad de generalización, pero que funcionan como una caja negra y requieren grandes recursos computacionales.

Hemos presentado el campo del procesamiento de lenguaje natural, incluyendo una revisión del estado de la cuestión, y sus retos y técnicas únicos. La imposibilidad de formalizar los lenguajes naturales, su complejidad y su ambigüedad son retos formidables, que aún estamos lejos de dominar. Los \textit{transformers} son actualmente el mejor mecanismo para abordar estos problemas: superan las limitaciones de las arquitecturas recurrentes previas y ofrecen una solución escalable, que puede usarse para construir \textit{grandes modelos de lenguaje}, redes neuronales capaces de desenvolverse eficazmente en problemas muy diversos.

Los \textit{transformers} son redes secuenciales que utilizan el mecanismo de atención para captar la relación entre símbolos, de forma flexible y elegante. Como hemos recalcado, la atención no es una técnica concreta sino más bien una familia de técnicas que puede adaptarse a multitud de problemas, como es el caso del estimador de Nadaraya-Watson, utilizado en problemas de regresión.

Además de potentes herramientas, los \textit{transformer} son modelos sencillos, construidos modularmente mediante bloques decodificador y codificador. Nos hemos esforzado por proporcionar una descripción clara y precisa del funcionamiento de los \textit{transformer} y de cómo integran técnicas como el uso de representaciones densas o la arquitectura codificador-decodificador, a fin de ofrecer una introducción al campo al lector sin conocimientos previos sobre aprendizaje automático.

El diseño modular permite crear arquitecturas especializadas, de tipo codificador (como Bert) o decodificador (como GPT). Además de los esquemas y descripciones en pseudocódigo presentados en el \cref{chapter5}, el \cref{appendixA} presenta una implementación completa de la familia GPT y el código necesario para entrenarla y evaluarla.

Modelos como GPT-1 o GPT-2 pueden utilizarse para abordar problemas complejos como la generación de texto en el estilo de Lope de Vega, estudiada en el \cref{chapter6}. Este ejercicio no solo muestra claramente el poder de los \textit{transformer}, sino que presenta la multitud de problemas asociados al entrenamiento de modelos \textit{transformer}: la necesidad de utilizar un conjunto de entrenamiento adecuado, de elegir adecuadamente los métodos y parámetros de aprendizaje, prevenir el sobreajuste…

Como ya hemos visto, el uso de grandes modelos pre-entrenados \textit{afinados} ofrece una forma particularmente eficaz y computacionalmente muy eficiente de aproximar estos problemas. Además, las posibilidades interpretativas que ofrece el mecanismo de atención, permite obtener intuiciones sobre su funcionamiento concreto.

\section{Campos relacionados}
El contenido presentado en este trabajo dista de ser una descripción exhaustiva sobre los \textit{transformer} y sus posibilidades. Presentamos ahora algunas vías de trabajo que pueden resultar de interés para el lector atraído por el tema.

En primer lugar, estas arquitecturas pueden abordar con resultados excelentes prácticamente cualquier problema relacionado con el lenguaje natural, desde la traducción automática al análisis de sentimientos en textos. La creación de modelos basados en \textit{transformer} (y técnicas de entrenamiento de los mismos) potentes, alineados o replicables es, como ya hemos comentado, objeto de una intensa investigación.

Otra línea de investigación es el uso de arquitecturas derivadas de los \textit{transformer} en otros campos, como la visión por computador, donde los \textit{vision transformer} han desplazado a las redes convolucionales. Invitamos al lector a que, con la base ofrecida en las páginas anteriores, explore sus aplicaciones.

El campo no solo ofrece multitud de retos prácticos, también siguen abiertos problemas teóricos muy trascendentales, como el cálculo eficiente de la atención o la comprensión y prevención de las \textit{alucinaciones}, problema este último especialmente problemático para la integración de los \textit{transformers} en herramientas de uso público. Se trata de un campo con una activa comunidad investigadora, con frecuentes descubrimientos de relevancia.

\section{Trabajo futuro}
El trabajo realizado en estas páginas ofrece una base para investigaciones futuras. En lo referente al problema de generar texto al estilo de Lope de Vega, utilizar modelos pre-entrenados más grandes y un conjunto de datos mayor muy probablemente proporcionará mejores resultados. Un conjunto de datos anotado podría permitir que la red neuronal pudiese realizar acciones adicionales, como generar una escena a partir de una descripción de la misma o relacionarse de forma interactiva con el usuario en el estilo del Siglo de Oro, de forma similar a un asistente conversacional. 

Otra posibilidad es aplicar esta técnica a otros autores  (necesariamente prolíficos, dada la importancia de un conjunto de datos suficientemente grande) e identificar qué características le cuesta más imitar al modelo. Es razonable esperar que características  locales, como el vocabulario utilizado, serán aprendidas de forma más fácil que características globales, como mantener y desarrollar un tema uniformemente.

Se abren, en definitiva, multitud de líneas de estudio, a buen seguro capaces de cautivar la atención de lectores de muy variados intereses.