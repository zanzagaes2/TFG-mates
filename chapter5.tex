\chapter{\textit{Transformers} y \textit{GPT}} \label{chapter5}
Tenemos la práctica totalidad de elementos necesarios para la presentación de los \textit{transformers}, arquitectura que presentamos en este capítulo. También presentamos arquitectura GPT, variante del \textit{transformer} utilizada para la generación de texto.

\section{La arquitectura \textit{transformer}}
\begin{figure}[tb]
    \centering
    \includesvg[scale=.8]{figures/chapter5/transformer.svg}
    \caption{Diagrama del diseño del \textit{transformer}. La anotación \( n × \)(\( × n \)) representa que existen \( n \) copias del componente, conectadas secuencialmente. Recordemos que en la traducción de un texto, el contexto sería el texto en el idioma original y la entrada sería el fragmento de texto ya traducido.}
    \label{fig:transformer}
\end{figure}

\begin{algorithm}[tb]
    \SetAlgoLined
    \KwData{$z, x \in \mathbb{V}^*$, secuencias de símbolos con longitud \( l_z, l_x\)}
    \KwResult{$P \in (0, 1)^{|\mathbb{V}| × |x|}$ donde $P_{i, j} = \widehat{P}\left( x[j + 1] = i \mid x[1:j], z; \theta \right) $}
    \KwHyper{$l_\text{max}$: longitud máxima de secuencia}
    \mydata{$L_\text{enc}, L_\text{dec}$, número de bloques codificador y decodificador}
    \mydata{$H$ número de cabezales de atención}
    \mydata{$d_e, d_\text{mlp}$ dimensión del espacio latente y de las capas neuronales}
    \KwParams{$\theta$ que incluye}
    \mydata{$W^e \in \real{d_e × |\mathbb{V}|}, W^p \in \real{d_e × l_\text{max}}$, matrices de \textit{tokenización}}
    \mydata{para $l = 1, …, L_\text{enc}$}
    \mydatal{$\mathfrak{W}_l^\text{enc}$, parámetros de la atención multicabezal del codificador}
    \mydatal{$\gamma_l^1, \gamma_l^2, \beta_l^1, \beta_l^2 \in \real{d_e}$, parámetros de normalización de capa}
    \mydatal{$W_l^1 \in \real{d_\text{mlp} × d_e}, b_l^1 \in \real{d_\text{mlp}}, W_l^2 \in \real{d_e × d_\text{mlp}}, b_l^2 \in \real{d_e}$, parámetros  neuronales}
    \mydata{para $l = 1, …, L_\text{dec}$}
    \mydatal{$\mathfrak{W}_l^\text{dec}$, parámetros de la atención multicabezal del decodificador}
    \mydatal{$\mathfrak{W}_l^\text{e/d}$, parámetros de la atención multicabezal cruzada}
    \mydatal{$(\gamma_l^i, \beta_l^i)_{i = 3}^5$, parámetros de normalización de capa}
    \mydatal{$W_l^3 \in \real{d_\text{mlp} × d_e}, b_l^3 \in \real{d_\text{mlp}}, W_l^4 \in \real{d_e × d_\text{mlp}}, b_l^4 \in \real{d_e}$, parámetros  neuronales}

    \tcc{Se tokeniza el contexto}
    $Z \gets [e_1, …, e_{l_z}]$ con $e_t \gets W^e[:, z[t]] + W^p[:, t]$\;
    \For{$l = 1, …, L_\text{enc}$}{
        $Z \gets Z + \multihead(Z, Z, Z; \mathfrak{W}_l^\text{enc})$\;
        para $t = 1, …, l_z$: $Z[:, t] \gets \LN(Z[:, t]; \gamma^1_l, \beta^1_l)$\;
        $Z \gets Z + W_l^2 \relu(W_l^1 Z + b_l^1 \mathbf{1}^T ) + b_l^2 \mathbf{1}^T $\;
        para $t = 1, …, l_z$: $Z[:, t] \gets \LN(Z[:, t]; \gamma^1_l, \beta^1_l)$\;
    }

    \tcc{Se decodifica la entrada utilizando el contexto}
    $X \gets [e_1, …, e_{l_x}]$ con $e_t = W^e[:, x[t]] + W^p[:, t]$\;
    \For{$l = 1, …, L_\text{dec}$}{
        Definimos $M \in \real{l_x × l_x}$ tal que $M_{ij} = \llbracket i \leq j \rrbracket$\;
        $X \gets X + \mmultihead(X, X, X, M; \mathfrak{W}_l^\text{dec})$\;
        para $t = 1, …, l_x$: $X[:, t] \gets \LN(X[:, t]; \gamma^3_l, \beta^3_l)$\;
        $X \gets X + \multihead(X, Z, Z; \mathfrak{W}_l^\text{e/d})$\;
        para $t = 1, …, l_x$: $X[:, t] \gets \LN(X[:, t]; \gamma^4_l, \beta^4_l)$\;
        $X \gets X + W_l^4 \relu(W_l^3 Z + b_l^3 \mathbf{1}^T ) + b_l^4 \mathbf{1}^T $\;
        para $t = 1, …, l_x$: $X[:, t] \gets \LN(X[:, t]; \gamma^5_l, \beta^5_l)$\;
    }
    \Return{$P = \softmax(W_u X)$}
    \caption{\textit{Transformer}\cite{phuong2022formal}}
    \label{algo:transformer}
\end{algorithm}

Como ya hemos dicho, la innovación de los \textit{transformer} proviene de integrar el mecanismo de atención en una red retro-alimentada de tipo codificador-decodificador para crear modelos de lenguaje secuencia a secuencia. En lugar de convoluciones o recurrencias, se utilizan capas de atención con función de similitud de producto escalar normalizado, computado como en la \cref{eq:dot_product}.

El \textit{transformer} recibe dos secuencias de símbolos: la secuencia de entrada, consumida por el bloque decodificador, y la secuencia de \textit{contexto}, consumida por el bloque codificador. El codificador utiliza atención bidireccional para crear una representación de la secuencia de contexto, que será consumida por el codificador junto a la secuencia de entrada para generar la salida del modelo.

La \cref{fig:transformer} muestra el diseño  de un \textit{transformer}. El codificador está compuesto por una pila de bloques idénticos, cada uno compuesto por dos capas: una capa de \textit{atención multicabezal} y una red neuronal retro-alimentada. Existe una conexión residual entre ambas capas que es inmediatamente seguida por una normalización de capa.

Los bloques están organizados secuencialmente, de forma que la salida de cada bloque del codificador está conectada a la entrada del bloque siguiente. La entrada del primer bloque es la secuencia de entrada \textit{tokenizada} presentada en la sección anterior: esta representación será proyectada en cada cabezal utilizando proyecciones aprendidas para crear las consultas y la base de datos. 

El decodificador tiene un diseño similar: se trata de una serie de bloques conectados entre sí, cada uno formado por tres capas. La primera capa es una atención multicabezal con máscara, que recibe la salida ya generada. Cuando se entrena utilizando \textit{teacher forcing}, el decodificador recibe la salida objetivo, pero solo atiende a las posiciones que el decodificador ya ha generado, comportándose como un modelo auto-regresivo.

La capa intermedia es la que se conoce como la \textit{atención codificador-decodificador}. En esta capa, las consultas se generan proyectando la salida del bloque previo del decodificador, mientras que la base de datos proviene de la salida del codificador. La última capa de cada bloque decodificador es una red neuronal retro-alimentada como la anteriormente descrita. De nuevo, todas las capas presentan conexiones residuales, seguidas de una normalización de capa.

La salida del último bloque decodificador es procesada por una red neuronal, que generará la salida esperada del modelo. A fin de formalizar el diagrama, el \cref{algo:transformer} presenta el funcionamiento de un \textit{transformer} en pseudocódigo.

\section{Modelos decodificador: GPT}
\begin{figure}[tb]
    \centering
    \includesvg[scale=.8]{figures/chapter5/gpt.svg}
    \caption{Diagrama del diseño de \textit{GPT}. La anotación \( n × \)(\( × n \)) representa que existen \( n \) copias del componente, conectadas secuencialmente.}
    \label{fig:gpt}
\end{figure}

El codificador y el decodificador presentes en el diseño del \textit{transformer} pueden utilizarse por separado para abordar problemas en los que no está presente una secuencia de contexto, como en la generación de texto. 

Un ejemplo son los modelos generativos decodificadores, que únicamente utilizan bloques de tipo decodificador, diseñados para generar una secuencia de símbolos a partir de un modelo de lenguaje aprendido y de los símbolos ya generados. Una de las familias de modelos generativos más influyentes es GPT, nacida en 2018 con la creación de GPT-1 \cite{radford2018improving}.

En el momento de su creación, GPT-1, un modelo sencillo basado en doce bloques de tipo decodificador encadenados, superó ampliamente a los modelos existentes en diversas tareas, como el razonamiento general o la respuesta a preguntas. Una innovación fundamental fue el uso de aprendizaje no supervisado: en lugar de depender íntegramente de bases de datos etiquetados como era habitual, GPT-1 fue entrenado de forma no supervisada sobre grandes \textit{corpus} de texto. La red luego puede reentrenarse (\textit{finetuning}) de forma supervisada sobre conjuntos etiquetados para adaptarla a tareas específicas.

\begin{algorithm}[tb]
    \SetAlgoLined
    \KwData{$x \in \mathbb{V}^*$, secuencias de símbolos con longitud \( l_z, l_x\)}
    \KwResult{$P \in (0, 1)^{|\mathbb{V}| × l_x}$ donde $P_{i, j} = \widehat{P}\left( x[j + 1] = i \mid x[1:j], z; \theta \right) $}
    \KwHyper{$l_\text{max}$: longitud máxima de secuencia}
    \mydata{$L_\text{dec}$, número de bloques decodificador}
    \mydata{$H$, número de cabezales de atención}
    \mydata{$d_e, d_\text{mlp}$ dimensión del espacio latente y de las capas neuronales}
    \KwParams{$\theta$ que incluye}
    \mydata{$W^e \in \real{d_e × |\mathbb{V}|}, W^p \in \real{d_e × l_\text{max}}$, matrices de \textit{tokenización}}
    \mydata{para $l = 1, …, L_\text{dec}$}
    \mydatal{$\mathfrak{W}_l^\text{dec}$, parámetros de la atención multicabezal}
    \mydatal{$\gamma_l^1, \beta_l^1, \gamma_l^2, \beta_l^2 \in \real{d_e}$, parámetros de normalización de capa}
    \mydatal{$W_l^1 \in \real{d_\text{mlp} × d_e}, b_l^1 \in \real{d_\text{mlp}}, W_l^2 \in \real{d_e × d_\text{mlp}}, b_l^2 \in \real{d_e}$, parámetros  neuronales}
    \mydatal{$\gamma, \beta$, parámetros de normalización de capa final}
    \mydatal{$W_u \in \real{|\mathbb{V}| \times d_e}$, inversa de la matriz de \textit{tokenización}}
  
    $X \gets [e_1, …, e_{l_x}]$ con $e_t = W^e[:, x[t]] + W^p[:, t]$\;
    \For{$l = 1, …, L_\text{dec}$}{
        \tcc{Definimos $M \in \real{l_x × l_x}$ tal que $M_{ij} = \llbracket i \leq j \rrbracket$}
        para $t = 1, …, l_x$: $\widetilde{X}[:, t] \gets \LN(X[:, t] \mid \gamma_l^1, \beta_l^1)$\;
        $X \gets X + \mmultihead(\widetilde{X}, \widetilde{X}, \widetilde{X}, M; \mathfrak{W}_l^\text{dec})$\;
        para $t = 1, …, l_x$: $\widetilde{X}[:, t] \gets \LN(X[:, t] \mid \gamma_l^2, \beta_l^2)$\;
        $X \gets X + W_l^2 \gelu(W_l^1 X + b_l^1 \mathbf{1}^T ) + b_l^2 \mathbf{1}^T $\;
    }
    para $t = 1, …, l_x$: $X[:, t] \gets \LN(X[:, t] \mid \gamma, \beta)$\;
    \Return{$P = \softmax(W_u X)$}
    \caption{$\texttt{GPT}(x \mid \theta)$\cite{phuong2022formal}}
    \label{algo:gpt}
  \end{algorithm}

La arquitectura GPT se presenta en la \cref{fig:gpt} y se describe formalmente en el \cref{algo:gpt}. Los bloques de GPT funcionan de forma prácticamente idéntica a los bloques decodificador del \textit{transformer}, salvo por la desaparición de la capa asociada a la atención cruzada y la sustitución de $\relu$ por $\gelu$ como función de activación \footnote{Sea \( \Phi(x) \) la función de distribución de la normal tipificada, es decir \( \Phi(x):= P(X\le x) \), con \( X \sim N(0,1) \). Se define GELU (\textit{Gaussian Error Linear Unit}) como $\gelu(x) \equiv x \cdot \Phi(x)$.}.

Solo un año después de la publicación de GPT-1, se presentó GPT-2, un modelo con 1.5 mil millones de parámetros que demostró por primera vez de forma clara la aparición de habilidades emergentes \cite{radford2019language}. La arquitectura de GPT-2 es prácticamente una versión escalada de GPT-1 con más parámetros y mayor longitud de contexto, salvo por cambios mínimos en la inicialización de los parámetros y en la forma de normalizar las capas. Aun así, supera a GPT-1 (y a la práctica mayoría de modelos existentes en ese momento) en una variedad de tareas, incluyendo el resumen de textos, la traducción de textos, el razonamiento usando sentido común o la comprensión lectora.

Como parte de este trabajo se ha realizado una implementación completa de la familia GPT, que puede encontrarse en el \Cref{appendixA}. Se ha utilizado un diseño modular, que puede utilizarse para generar GPT-1 y GPT-2.

\subsection{Generación de texto} \label{sec:gen}
Los modelos de lenguaje computados por los modelos \textit{transformer} o GPT pueden utilizarse para abordar prácticamente cualquier problema de procesamiento de lenguaje natural. Discutimos ahora cómo utilizar un modelo de lenguaje para generar texto. 

\begin{definition}[Proceso de generación de texto]
    Dado un lenguaje \( (\mathbb{V}^*, \mathbb{P}) \), un modelo de lenguaje \( \widehat{\mathbb{P}} \) de \( \mathbb{V^*} \) y un parámetro (de temperatura) \( \tau \in (0, \infty) \) llamamos \textit{proceso de generación de texto} a la sucesión de variables aleatorias  \( \Set{X_k}_{k = 0}^\infty \) con valores en \( \mathbb{V} \) tales que:
    \begin{equation} \label{eq:generation}
        \mathbb{P}(X_{k + 1} = \alpha \mid X_{0}, …, X_{k}) = \widehat{\mathbb{P}} \left( \alpha \mid  X_0, …, X_k \right)^{1/\tau} \text{ para k = 0, 1, … }
    \end{equation}
\end{definition}

\begin{algorithm}[tb]
    \SetAlgoLined
    \KwData{$\theta$, parámetros (tras el entrenamiento)}
    \mydata{$x \in \mathbb{V}^*$, texto de estímulo con longitud $l_x$}
    \KwHyper{$\tau \in (0, \infty)$, temperatura}
    \mydata{$l_\text{gen} \in \nat$, símbolos generados}
    \KwResult{$y \in \mathbb{V}^*$, texto generado}
  
    \For{$i = 1, …, l_x$} {
      $P \gets GPT(x; \theta)[:, l_x + i - 1]$ \;
      Tomar una muestra de una variable $Y$ con $\mathbb{P}(Y) \propto P^{1 / \tau}$ \;
      $x \gets [x, y]$ \;
    }
    \Return{$y = x[l_x + 1: l_x + l_\text{gen}]$}
  
    \caption{Generación de texto usando GPT \cite{phuong2022formal}}
    \label{algo:gpt_inference}
  \end{algorithm}
  
El parámetro de temperatura permite ajustar el proceso de generación de texto: valores mayores de este parámetro provocarán la generación de textos más diversos. Notemos que la \cref{eq:generation} no impone ninguna condición sobre \( X_0 \): es frecuente fijar su valor a una constante \( \gamma \) (imponiendo \( \mathbb{P}(X_0 = \gamma) = 1 \)), que recibe el nombre de \textit{texto de estímulo} (\textit{prompt}). El \cref{algo:gpt_inference} describe el proceso de generación de texto usando GPT.

Nótese que en un modelo sin memoria la \cref{eq:prob} implica que la sucesión \( \Set{X_k} \) se comporta como una cadena de Markov de orden \( l_\text{máx} \). En particular, la sucesión de variables aleatorias \( \Set{Y_k}_{k = 0}^{\infty} \) con valores en \( \mathbb{V}^{l_\text{máx}} \) definidas como:
\[
    Y_k = X_k, …, X_{k + {l_\text{máx}}}
\]
es una cadena de Markov de primer orden con matriz de transición \( M \) de dimensiones \( |\mathbb{V}|^{l_\text{máx}} \times |\mathbb{V}|^{l_\text{máx}} \). Aunque esta matriz es intratable (por ejemplo, con los parámetros de GPT-3 la matriz \( M \) tendría \( 10^{19256} \) entradas) es extremadamente dispersa. En particular, dados dos estados \( Y^{(1)}, Y^{(2)} \) de la forma:
\begin{align*}
    Y^{(1)} = X_1^{(1)}, …, X_{l_\text{máx}}^{(1)} &&
    Y^{(2)} = X_1^{(2)}, …, X_{l_\text{máx}}^{(2)}
\end{align*}
una condición necesaria para que la entrada de \( M \) asociada a \( (Y^{(1)},  Y^{(2)}) \) sea distinta de \( 0 \) es que:
\begin{align*}
    X_1^{(1)}, …, X_{l_\text{máx} - 1}^{(1)} = X_2^{(2)}, …, X_{l_\text{máx}}^{(2)} && \left(\text{ o bien } X_1^{(2)}, …, X_{l_\text{máx}}^{(1)} = X_1^{(2)}, …, X_{l_\text{máx} - 1}^{(2)} \right)
\end{align*}
lo que (asumiendo equidistribución de los elementos de \( \mathbb{V} \)) solo sucede con probabilidad \( \left( \frac{2}{|\mathbb{V}|} \right)^{l_\text{máx} - 1} \).

El poder expresivo de los \textit{transformer} proviene de la capacidad para almacenar de forma extremadamente eficiente esta matriz, explotando el alto grado de dispersión. 